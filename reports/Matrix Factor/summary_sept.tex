\documentclass[12pt,a4paper]{article}
\usepackage{CJK, CJKpunct}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{listings}
\lstset{language=C++}
\lstset{breaklines}
\lstset{extendedchars=false}
\usepackage{indentfirst}
\usepackage{amsfonts,amssymb}
\usepackage[numbers]{natbib}
\usepackage{tikz}

\newcommand{\song}{\CJKfamily{song}}
\newcommand{\hei}{\CJKfamily{hei}}
\newcommand{\kai}{\CJKfamily{kai}}
\newcommand{\fs}{\CJKfamily{fs}}

\begin{document}
\begin{CJK*}{UTF8}{song}

\title{Matrix Factorization Method}
\author{朱恬骅 \\
\small{09300240004 计算机科学与技术}}
\date{\today}
\maketitle

\section{Background}
Under many circumstances two or more prospectives are possible for observing one series of objects. There might be several situations about the dependence existing amonng different observations. In this paper, we will propose an approach to cope with the following two situations: First, observation data from two perspectives are both available, and we want to utilitize them for possible clusering or classification accuracy improvement. Second, some parts of the data are missing, where just one perspective is available, say X, and we want to estimate the observation for other perspective Y, in order to utilitize an existing and good clustering/classification method on Y.

In this paper, our data is extracted from the Chinese stock market, and the perspectives involved includes (1) price data, (2) news about every stock, and (3) business sphere description.

\section{Matrix Factorization Method}
\subsection{Generative Model}
We will discuss the first situation first, that is, to exploit observations obtained from two perspectives, to enhance clustering/classification precision. In topic models, we have assumed that a document is generated from several chosen topics, with appropriate proportion. Hierarchical topic models have assumed farther that some hierarchical structure exists among these topics, and Dirichlet process is introduced in order to solve situations involving infinite or undetermined topics.

In this paper, we combine the idea of hierarchical topic models with normal model. Our assumption is based on the following fact: an object itself is the determinant factor in generating the observations under different perspectives. An individual object, or in our context, stock, is the explicit factor that differs among every possible datum.

To simplify our discussion, we will first consider situations where two perspectives are involved. Denote $X$ for the observations results from the first perspective and $Y$ for the second, and we have correspondingly two sets of topics, named $W$ and $Z$, who are latent random variables that determined a distribution over the word frequency in every document, i.e. $X$ and $Y$. The two sets of topics are by iteslf independent, for that we have no physically meaningful measures to apply on the price data and news text. Every generative process, from $W$ to get $X$, or draw $Y$ from $Z$, are independent. The de facto dependence between $X$ and $Y$ is caused by the way we draw $W$ and $Z$ and assign proportions to them.

In latent Dirichet allocation, we have known that $W$ and $Z$ each are determined by its Dirichlet parameter, namely $\alpha$s. Here we introduce a hidden random variable, $h$, which served as a determinant for choosing appropriate proportion over $W$ and $Z$.In the reality, $h$ is the abstraction from objects. Thus, the clustering of observation $(X, Y)$ is actually the clustering over different draws from $h$.

\subsection{Matrix Factorization Method}
Tranditional topic model may also assume that, two observations, or two sets of feature properties, are results of independent topics. Further more, any two topics are irrelevant. However, in the reality, such constraints seldom hold. Two topics may cover a mutual set of conditions, and two obseravtions, if they are correspondent to one same object, may interfere each other, or display some mutual features. In such situations, we cannot adopt independent identical distribution assumption any longer.

Instead, we introduce a matrix, called topical co-occurence matrix. This matrix, denoted as $A$ in the following sections, describes the posibility of two topics from distinct views occuring in the same context. That is, the relevance between $X$s and $Y$s are assumed to attribute to the relevance between their own generative topics.

Given some observed vectors $X_1, X_2, ..., X_n \in \mathbb{R}^p$ and $Y_1, Y_2, ..., Y_n \in \mathbb{R}^q$, each $X_i$ and $Y_i$ are features for one object, we want to reduce the number of representation dimensions, by using its topic information. For the reason that topic variables are hidden, thus inaccessible to the observers, we can only estimate the matrix $A$ by factorizing the expectation matrix of the product of the two observation vectors, i.e. $E = \mathbf{E}XY^\top$. In our model, we have two sets of topics, say $T_X$ and $T_Y$, and note $s = |T_X|$, $t = |T_Y|$, generating results according to the topic-result possiblity matrices $P_X$ and $P_Y$ (and sometimes, generating several results, and the observation is the sum). Then we have the following equation and constraints held:

\begin{eqnarray}
E \simeq P A Q^\top \\
\sum_{i,j} A_{ij} = 1 \\
\sum_{j} P_{ij} = 1, \forall i \\
\sum_{j} Q_{ij} = 1, \forall i
\end{eqnarray}

where $A \in \mathbb{R}^{s \times t}$, and $p, q$ are the length of observation vectors from $X$ and $Y$, repectively. In order to dispel scale difference, $X$s and $Y$s are normalized to $norm(\centerdot, 1) = 1$.

\subsection{Estimating matrix $A$}
We denote words domain in $X$ as $\Sigma = \{\sigma_1, \sigma_2, \sigma_3, ...\}$ and that in $Y$ as $T = \{\tau_1, \tau_2, \tau_3, ... \}$, and the probability of observing $\sigma_i$ in $X$ and $\tau_j$ in the corresponding $Y$ is estimated by $E_{ij} = \frac{1}{N} \sum_{k=1}^{N}{X_i^{(k)} Y_j^{(k)}}$.

Assume that the distribution of word $\sigma_i$ on topic $w \in W$ is given by $p_X(i|w)$, and $p_Y(j|z)$ depicted word $\tau_j$ on topic $z \in Z$, we have $E_{ij} = p_X(i|w) p(w, z) p_Y(j|z)$, where $p(w, z)$ denotes the topical co-occurence probability of $w$ and $z$, which is obtainable from $A$.

It is evident that any distribution of $p_X(i|w)$ or $p_Y(j|z)$ is possible, amongst which we consider multinomial as the most appropriate one. The distribution may degenerate to a skew distribution when the dictionary, or say $\sigma$s and $\tau$s are not appropriately chosen, and to a uniform distribution when the dictionary is built on some keywords.

The distribution of $p_X(i|w)$ and $p_Y(j|z)$ can be estimated by using EM method, which can form matricies $P$ and $Q$. Then we can get $A$ by calculatng
\begin{equation}
A = \arg \min_A S(E - P A Q^\top)
\label{equ_est_A}
\end{equation}
where $S$ the square error function.

\subsection{Describing $h$ by using $A$}
For the situations where multiple observations for each object is possible, we can get several $(X, Y)$ from one same object. Denote its mean observation as $(\bar{X}, \bar{Y})$. To exploit them, we can use $A$ to depict $h$, whereby at the same time elimate differences in observation samples. In this case, we use the globally trained $P$ and $Q$, which is obtained from $\bar{X}$s and $\bar{Y}$s. We calculates $E$ using the observations of the same object, and $A$ for this object can be also estimated by using equation \ref{equ_est_A}. Hence, we have a matrix to indicate the object, and such matricies resembles when their owner objects are similar. We may then adopt clustering or classifciation over these $A$ matricies, by using 3D k-means or other tensor methods. $A$ is the distribution by which we draw $w$ and $z$ from the topic sets $W$ and $Z$.

\subsection{Knowledge transfering}
In this subsection, we will discuss the second problem, that is, to estimate reasonable values for the missing perspective $Y$ from available data $X$.

Since we have estimated the probability of $\sigma_i$ co-occurring with $\tau_j$ as in matrix $E$, and $P$ and $Q$ for distribution over words given some topic, it is easy to estimate $p(w, z)$ by adopting equation \ref{equ_est_A}. Hence, for the incoming observation $X = (p(\sigma_1), p(\sigma_2), p(\sigma_3), ...)$, our estimation for the probability of the unseen observation $Y$ is generate by topic $z$, i.e., $p_Y(z|X)$ is given by the following equation:

\begin{equation}
p_Y(z|X) = \sum_w \frac{p(w|X)p(w, z)}{p_Y(z)}
\end{equation}

And we may further imply the possible $Y$ is that $\{\sum_z p_Y(z|X)p_Y(j|z)\}_{j=1}^q$. In many cases where topics are used to reduce dimension, using $p_Y(z|X)$ is more suitable.

\section{Dataset}
\subsection{Small-scale dataset}
Select four stocks from each of banking, pharmaceutical and steel industries, to compose the small-scale dataset, named S12.
\begin{multicols}{2}
\begin{enumerate}
\item Banking
\subitem 600000 浦发银行 Pufa Bank
\subitem 600015 华夏银行 Huaxia Bank
\subitem 600016 民生银行 Minsheng Bank
\subitem 600036 招商银行 Zhaoshang Bank
\item Pharmaceutical
\subitem 600085 同仁堂 Tongrentang
\subitem 600129 太极集团 Taiji Group
\subitem 6000422 昆明制药 Kunming Pharm
\subitem 000423 东阿阿胶 Dong'e Ejiao
\item Steel
\subitem 600019 宝钢股份 Baosteel
\subitem 000959 首钢股份 Shougang Steel
\subitem 000709 河北钢铁 Hebei Steel
\subitem 600569 安阳钢铁 Anyang Steel
\end{enumerate}
\end{multicols}

\subsection{Medium-scale dataset}
\subsubsection{50 stocks from 5 irrelevant industries}
Noted as S50.
\begin{multicols}{2}
\begin{enumerate}
\item Banking
\subitem 000001 深发展A
\subitem 002142 宁波银行
\subitem 600000 浦发银行
\subitem 600015 华夏银行
\subitem 600016 民生银行
\subitem 600036 招商银行
\subitem 601009 南京银行
\subitem 601166 兴业银行
\subitem 601169 北京银行
\subitem 601288 农业银行
\item Steel
\subitem 000629 攀钢钒钛
\subitem 000709 河北钢铁
\subitem 000717 韶钢松山
\subitem 000898 鞍钢股份
\subitem 000932 华菱钢铁
\subitem 600282 南钢股份
\subitem 600019 宝钢股份
\subitem 000959 首钢股份
\subitem 000022 济南钢铁
\subitem 600569 安阳钢铁
\item Pharmaceutical
\subitem 601607 上海医药
\subitem 600511 国药股份
\subitem 600833 第一医药
\subitem 600713 南京医药
\subitem 000028 国药一致
\subitem 600085 同仁堂
\subitem 600129 太极集团
\subitem 600422 昆明制药
\subitem 000423 东阿阿胶
\subitem 002589 瑞康医药
\item Wine and Alcoholic Drinks
\subitem 000568 泸州老窖
\subitem 000858 五粮液
\subitem 600519 贵州茅台
\subitem 600779 水井坊
\subitem 000596 古井贡酒
\subitem 600809 山西汾酒
\subitem 000799 酒鬼酒
\subitem 002304 洋河股份
\subitem 600702 沱牌舍得
\subitem 600559 老白干酒
\item Software Development
\subitem 600570 恒生电子
\subitem 600756 浪潮软件
\subitem 000948 南天信息
\subitem 600271 航天信息
\subitem 002063 远光软件
\subitem 002065 东华软件
\subitem 000938 紫光股份
\subitem 002073 软控股份
\subitem 002090 金智科技
\subitem 002230 科大讯飞
\end{enumerate}
\end{multicols}

\subsubsection{50 stocks from 5 relevant industries}
Noted as R50.
\begin{multicols}{2}
\begin{enumerate}
\item Steel
\subitem 000629 攀钢钒钛
\subitem 000709 河北钢铁
\subitem 000717 韶钢松山
\subitem 000898 鞍钢股份
\subitem 000932 华菱钢铁
\subitem 600282 南钢股份
\subitem 600019 宝钢股份
\subitem 000959 首钢股份
\subitem 000022 济南钢铁
\subitem 600569 安阳钢铁
\item Charcoal
\subitem 000780 平庄能源
\subitem 000723 美锦能源
\subitem 002128 露天煤业
\subitem 600188 兖州煤业
\subitem 600348 阳泉煤业
\subitem 600546 山煤国际
\subitem 600740 山西焦化
\subitem 601001 大同煤业
\subitem 601666 平煤股份
\subitem 601898 中煤能源
\item Vehicle manufacturing
\subitem 000550 江铃汽车
\subitem 000572 海马汽车
\subitem 000625 长安汽车
\subitem 000800 一汽轿车
\subitem 000868 安凯客车
\subitem 000927 一汽夏利
\subitem 000951 中国重汽
\subitem 000957 中通客车
\subitem 002594 比亚迪
\subitem 600006 东风汽车
\item Airlines and Aircraft Industry
\subitem 600029 南方航空
\subitem 600115 东方航空
\subitem 600221 海南航空
\subitem 600316 洪都航空
\subitem 000089 深圳机场
\subitem 600004 白云机场
\subitem 600009 上海机场
\subitem 600151 航天机电
\subitem 600893 航空动力
\subitem 000901 航天科技
\item Power
\subitem 600011 华能国际
\subitem 600021 上海电力
\subitem 600027 华电国际
\subitem 600644 乐山电力
\subitem 600101 明星电力
\subitem 600116 三峡水利
\subitem 600131 岷江水电
\subitem 600236 桂冠电力
\subitem 600292 九龙电力
\subitem 600310 桂东电力
\end{enumerate}
\end{multicols}

\subsection{Large-scale dataset}
\subsubsection{S100 and S500}
Select randomly 100 and 500 stocks, to compose the S100 and S500 dataset.

\subsection{Notes on time periods}
We denote the time period by adding a suffix composed of a hyphen followed by two digits indicating the starting year. The time period is always chosen from July 1 of that year to June 30 two years later. For example, S12-09 indicates the dataset contains information generated in the time period starting from July 1, 2009 to June 30, 2011, relating to the chosen 12 stocks.

\section{Experiments on single-view dataset}
\subsection{股价信息}
\subsubsection{表示法}
\paragraph{时序涨跌幅词频表示法}
将每一支股票的走势看作一篇文档。设每支股票取$T+1$天的价格信息，建立一个大小为$2T$的词汇表，包括了“第$i$天涨1\%”和“第$i$天跌1\%”，$i=2,3,...,T+1$。将股票的走势表现为这$2T$个词上的词频。

\paragraph{股票涨跌幅词频表示法}
将每一天的走势看作一篇文档，设共有$N$支股票，则建立一个大小为$2N$的词汇表，包括“第$i$支股票涨1\%”和“第$i$支股票跌1\%”，$i=1,2,...,N$。将每天的股票行情表示为这$2N$个词上的词频。

\paragraph{正负零收益表示法}
将每一支股票的走势看作一篇文档。设每支股票取$T+1$天的价格信息，建立一个大小为$3T$的词汇表，包括了“第$i$天涨”、“第$i$天持平”和“第$i$天跌”，$i=2,3,...,T+1$。将股票的走势表现为这$3T$个词上的词频。

\subsubsection{聚类方法}
\label{SSSAggregationMethods}
\paragraph{直接K-Means法}
对于选定的股价特征，直接运行K-Means。由于初始中心的随机性，运行多次，选取类与类之间分布最为平均的一次结果。

\paragraph{LDA法}
对选定的股价特征，运行LDA进行聚类，选取最可能属于的topic作为这支股票的类标记。

\paragraph{LDA + K-Means法}
对选定的股价特征，先运行LDA进行聚类；将该股票属于这些topic的可能性作为新的特征，运行K-Means进行聚类。

\subsection{文本信息（经营范围描述）}
\subsubsection{表示法}
\paragraph{全文词频表示法}
即对经营范围描述信息进行分词后，对所有出现的词都计算词频，是最简单的方法。

\paragraph{构建关键词词典}
为去除一些意义不大的高频词，需要构造一个比较干净的关键词词典。第一种方法是计算一个词的文档间频率DF及其对应的信息熵$H(w)$，进行降序排序，这就构建出了针对特定语料的关键词词典。以这一词典为基础统计的全文词频，将比在所有词或高频词的字典上统计得到的词频更能代表语料的特征。

\subsubsection{聚类方法}
同 \ref{SSSAggregationMethods} 。

\subsection{结果}
在所有2215支股票的经营范围描述文本信息中，采用构建关键词词典方法，查找出的前100个高频词如下：

\begin{multicols}{5}
经营\\
技术\\
销售\\
生产\\
业务\\
出口\\
设备\\
服务\\
开发\\
进出口\\
材料\\
许可\\
企业\\
项目\\
机械\\
工程\\
国家\\
商品\\
加工\\
咨询\\
除外\\
制造\\
本企业\\
投资\\
配件\\
电子\\
禁止\\
管理\\
公司\\
不含\\
进出口业\\
进出口业务\\
相关\\
许可证\\
化工\\
设计\\
代理\\
机械设备\\
系统\\
仪器\\
建筑\\
营本企业\\
计算机\\
经营本企业\\
金属\\
规定\\
信息\\
汽车\\
法律\\
仪表\\
法规\\
范围\\
货物\\
安装\\
公司经营\\
原辅\\
贸易\\
零配件\\
辅材料\\
所需的
\end{multicols}

在所有可能的词（组）中，信息熵最大的词（组）如下：

\begin{multicols}{5}
咨询\\
国家\\
材料\\
机械\\
除外\\
商品\\
禁止\\
项目\\
进出口\\
进出口业\\
服务\\
制造\\
业务\\
企业\\
开发\\
设备\\
公司\\
投资\\
管理\\
加工\\
不含\\
许可\\
许可证\\
工程\\
相关\\
电子\\
自产\\
设计\\
仪器\\
化工\\
产品\\
生产\\
代理\\
仪表\\
限定\\
技术\\
建筑\\
零配件\\
制品\\
范围\\
规定\\
进口\\
货物\\
租赁\\
安装\\
信息\\
房地产\\
计算机\\
及其\\
各类\\
法律\\
批发\\
法规\\
配件\\
自营\\
零售\\
汽车\\
限制\\
系统
\end{multicols}

然后使用贝叶斯平均方法提取了所有2215支股票经营范围描述的关键词。得到的部分结果如下：

000001  监管 人民币 汇款 借款 放款 非贸易 有价证券 汇兑 信托业 外币 见证 资信 承兑 各项 存款 贴现 调查 票据 结算
外汇 代理业 人民 保险 境内 买卖 允许 发行 有关 境外 办理\\
002142  十一 十三 十二 金融债 中国银 公众 银行卡 信用证 发放 中期 款项 长期 收付 兑付 短期 吸收 债券 监督 保险业
政府 承兑 银行 拆借 存款 担保 中国 贴现 保管 同业 票据\\
600000  外汇 托管 保险箱 全国 离岸 保障 外币 借款 汇款 兑换 委员会 社会 银行业 见证 资信 中国银 拆借 结汇 股票 存款
担保 公众 贴现 同业 信用证 发放 中期 款项 长期 收付\\
600015  金融债 委员会 中国银 结汇 公众 银行卡 债券 信用证 发放 款项 中期 长期 收付 兑付 短期 政府 吸收 监督 承兑
拆借 存款 担保 贴现 保管 同业 买卖 票据 结算 贷款 代理业\\
600016  本行 十四 十一 十三 十二 可以 银行业 结汇 金融债 公众 银行卡 信用证 发放 中期 款项 长期 收付 兑付 短期 吸收
监督 保险业 承兑 银行 拆借 债券 存款 担保 中国 政府\\
这几个都是银行股。

000028  医用 区域性 救护车 口腔科 化验 缝合 一次性 灭菌 诊断 同化 第一 器具 激素 手术室 急救室 精神 抗生素 射线
麻醉药 超声 敷料 附属 临床 诊疗 蛋白 毒性 疫苗 分析 消毒 合剂\\
000423  膏剂 合剂 糖浆 口服液 保健 颗粒剂 胶囊 药品 批准 食品 范围 许可证 进出口业 商品 生产 销售\\
002589  保存 常温 毒液 罂粟 助听器 隐形眼镜 同化 激素 体外 麻醉药 蛋白 毒性 疫苗 健身器 护理 诊断 抗生素 精神 配送
三类 化学药 饮片 日用品 生化 试剂 中药材 制毒 生物制品 中成药 化妆品\\
600085  营养液 老年病 乌鸡 作用 妇产科 儿科 梅花鹿 乌骨鸡 外科 冷食品 中医科 内科 马鹿 涂膜剂 同仁 皮肤科 供暖 定型
皮肤 北京 诊疗 其中 股份 动植物 西药 饲养 有限公司 图书 保健 饮片\\
600129  执业 中草药 旅馆 水产 西药 作业 二级 首饰 前不 副食品 保健 金银 土地 中成药 养殖 以下 工艺美术 维护 种植
经济 印刷 不得 百货 医疗 旅游 器械 出租 自有 化学 包装\\
这几个都是医药方面的。

但也有不好的例子，比如东华软件的：\\
002065  决定 国务院 机关 注册 选择 行政 自主 工商 登记 不得 活动 开展 批准 后方 法规 法律 审批 自营 规定 各类 限定
许可 代理 禁止 进出口业 公司 管理 商品 除外 进出口\\
完全没有体现出它经营范围是什么。



%\renewcommand\refname{References}
%\bibliography{main}
%\bibliographystyle{plain}

\end{CJK*}
\end{document}